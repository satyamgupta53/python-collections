{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "model = spacy.load(\"en_core_web_sm\", disable=['parser','ner']) # loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['upGrad', 'upgrad', 'NOUN', 'NN', False, '']\n",
      "['is', 'be', 'AUX', 'VBZ', True, '']\n",
      "['teaching', 'teach', 'VERB', 'VBG', False, '']\n",
      "['Data', 'Data', 'PROPN', 'NNP', False, '']\n",
      "['Science', 'Science', 'PROPN', 'NNP', False, '']\n",
      "['courses', 'course', 'NOUN', 'NNS', False, '']\n",
      "['to', 'to', 'ADP', 'IN', True, '']\n",
      "['working', 'work', 'VERB', 'VBG', False, '']\n",
      "['professionals', 'professional', 'NOUN', 'NNS', False, '']\n",
      "['.', '.', 'PUNCT', '.', False, '']\n"
     ]
    }
   ],
   "source": [
    "tokens = model(\"upGrad is teaching Data Science courses to working professionals.\")\n",
    "for token in tokens:\n",
    "    print([token.text, token.lemma_, token.pos_, token.tag_, token.is_stop, token.dep_], sep=\"\\t\") # lemma - lemmatize, is_stop - stop word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h3><b> Product Feature Extraction from Customer Reviews</b><br></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feel</td>\n",
       "      <td>feel</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>so</td>\n",
       "      <td>so</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LUCKY</td>\n",
       "      <td>LUCKY</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text  lemma   pos\n",
       "0      I      I  PRON\n",
       "1   feel   feel  VERB\n",
       "2     so     so   ADV\n",
       "3  LUCKY  LUCKY  NOUN\n",
       "4     to     to  PART"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos, lemma, text = [], [], []\n",
    "\n",
    "with open(\"F:\\programming\\python-development\\python-questions\\Database\\Samsung_reviews.txt\", 'rb') as file:\n",
    "    data = file.read().decode('utf-8').split(\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "trial_review = data[0]\n",
    "for tokens in model(trial_review):\n",
    "    text.append(tokens.text)\n",
    "    lemma.append(tokens.lemma_)\n",
    "    pos.append(tokens.pos_)\n",
    "    \n",
    "model_table = pd.DataFrame({'text': text, 'lemma': lemma, 'pos': pos})\n",
    "model_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lemma\n",
       "phone    3\n",
       "one      2\n",
       "LUCKY    1\n",
       "line     1\n",
       "year     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most frequently occuring num in review 1\n",
    "model_table[model_table['pos'] == 'NOUN']['lemma'].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 129.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "phone      1216\n",
       "time         90\n",
       "battery      90\n",
       "screen       87\n",
       "price        87\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# track progress while applying the same hypothesis on larger set of reviews\n",
    "from tqdm import tqdm\n",
    "nouns = []\n",
    "for review in tqdm(data[0:1000]):\n",
    "    doc = model(review)\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            nouns.append(token.lemma_.lower())\n",
    "pd.Series(nouns).value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that battery life',\n",
       " 'The battery was',\n",
       " 'great battery life',\n",
       " 'removable battery or',\n",
       " 'the battery in']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understanding cotext of the noun tags\n",
    "pattern = re.compile(\"\\w+\\sbattery\\s\\w+\")\n",
    "file_samsung = open(\"F:\\programming\\python-development\\python-questions\\Database\\Samsung_reviews.txt\", 'rb').read().decode('utf-8')\n",
    "pattern_matched = re.findall(pattern, file_samsung)\n",
    "type(pattern_matched)\n",
    "pattern_matched[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefixes</th>\n",
       "      <th>keywords</th>\n",
       "      <th>suffixes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>battery</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great</td>\n",
       "      <td>battery</td>\n",
       "      <td>lasts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>long</td>\n",
       "      <td>battery</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new</td>\n",
       "      <td>battery</td>\n",
       "      <td>runs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>removable</td>\n",
       "      <td>battery</td>\n",
       "      <td>drains</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    prefixes keywords suffixes\n",
       "0       good  battery     life\n",
       "1      great  battery    lasts\n",
       "2       long  battery     last\n",
       "3        new  battery     runs\n",
       "4  removable  battery   drains"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixes, suffixes = [], []\n",
    "for i in pattern_matched:\n",
    "    l = i.split(\" \")\n",
    "    prefixes.append(l[0].lower())\n",
    "    suffixes.append(l[-1].lower())\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "prefixes, suffixes = [p for p in prefixes if p not in stop], [s for s in suffixes if s not in stop]\n",
    "prefixes, suffixes = pd.Series(prefixes).value_counts().head(5).index, pd.Series(suffixes).value_counts().head(5).index\n",
    "\n",
    "pre_suf = pd.DataFrame({'prefixes': prefixes,'keywords':['battery']*len(prefixes), 'suffixes': suffixes})\n",
    "pre_suf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('He', 'PRON')\n",
      "('wished', 'VERB')\n",
      "('he', 'PRON')\n",
      "('was', 'VERB')\n",
      "('rich', 'ADJ')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Database/tagged_words.csv\")\n",
    "sent = \"He wished he was rich\"\n",
    "\n",
    "def get_common_tag(data,word):\n",
    "    if word.lower() in data['word'].unique():\n",
    "        q = f\"word=='{word.lower()}'\"\n",
    "        return word , data.query(q)['tag'].value_counts().head().index.tolist()[0]\n",
    "    else:\n",
    "        return f\"{word} not in data\"\n",
    "\n",
    "for word in sent.split(\" \"):\n",
    "    print(get_common_tag(data,word))\n",
    " \n",
    "data.query(\"word=='saw'\")['tag'].value_counts().head().index.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "DET     0.051\n",
       "PRON    0.001\n",
       "X       0.001\n",
       "Name: his, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_matrix = pd.crosstab(data['word'], data['tag'], normalize='columns')\n",
    "emission_matrix.loc['his'][emission_matrix.loc['his']>0].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h3><b>Parsing in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "parser = spacy.load(\"en_core_web_sm\", disable=['ner']) # loading pre-trained model\n",
    "\n",
    "active = ['Hens lay eggs.',\n",
    "'Birds build nests.',\n",
    "'The batter hit the ball.',\n",
    "'The computer transmitted a copy of the manual']\n",
    "\n",
    "passive = ['Eggs are laid by hens',\n",
    "'Nests are built by birds',\n",
    "'The ball was hit by the batter',\n",
    "'A copy of the manual was transmitted by the computer.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHE   nsubj\n",
      "IS   ROOT\n",
      "A   det\n",
      "FOOL   attr\n"
     ]
    }
   ],
   "source": [
    "# trial_line = parser(active[0])\n",
    "trial_line = parser(\"SHE IS A FOOL\")\n",
    "for token in trial_line:\n",
    "    print(token.text,\" \", token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         transmitted            \n",
      "    __________|_______           \n",
      "   |                 copy       \n",
      "   |           _______|_____     \n",
      "   |          |             of  \n",
      "   |          |             |    \n",
      "computer      |           manual\n",
      "   |          |             |    \n",
      "  The         a            the  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk library to print the constituent parse tree\n",
    "from nltk import Tree\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children]) # node.orth_ refers to the word\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in parser(\"The computer transmitted a copy of the manual\").sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"058063e1093e4c79b19761a67b919250-0\" class=\"displacy\" width=\"650\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">SHE</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">IS</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">A</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">FOOL</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-058063e1093e4c79b19761a67b919250-0-0\" stroke-width=\"2px\" d=\"M62,152.0 62,127.0 197.0,127.0 197.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-058063e1093e4c79b19761a67b919250-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,154.0 L58,146.0 66,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-058063e1093e4c79b19761a67b919250-0-1\" stroke-width=\"2px\" d=\"M362,152.0 362,127.0 497.0,127.0 497.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-058063e1093e4c79b19761a67b919250-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M362,154.0 L358,146.0 366,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-058063e1093e4c79b19761a67b919250-0-2\" stroke-width=\"2px\" d=\"M212,152.0 212,102.0 500.0,102.0 500.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-058063e1093e4c79b19761a67b919250-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M500.0,154.0 L504.0,146.0 496.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(trial_line, style='dep', options={'distance': 150, 'compact': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in active:\n",
    "#     displacy.render(parser(sentence), style='dep', options={'distance': 90, 'compact':True})\n",
    "\n",
    "# for sentence in passive:\n",
    "#     displacy.render(parser(sentence), style='dep', options={'distance': 90, 'compact':True})\n",
    "\n",
    "# to identify the passive sentences\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# matching dependency nsubjpass\n",
    "passive_rule = [{'DEP':{'IN':['nsubjpass', 'auxpass', 'csubjpass', 'agent']}}]\n",
    "matcher = Matcher(parser.vocab)\n",
    "matcher.add('Rule', [passive_rule])\n",
    "\n",
    "def is_passive(sentence):\n",
    "    doc = parser(sentence)\n",
    "    matches = matcher(doc)\n",
    "    if matches:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "for sentence in passive:\n",
    "    print(is_passive(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h3><b>Name Entity Recognition & CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. PROPN O \n",
      "Sumit PROPN B PERSON\n",
      "is AUX O \n",
      "an DET O \n",
      "adjunct ADJ O \n",
      "faculty NOUN O \n",
      "at ADP O \n",
      "UpGrad PROPN B ORG\n",
      ". PUNCT O \n",
      "--------------------------\n",
      "Sumit 3 8 PERSON\n",
      "UpGrad 34 40 ORG\n"
     ]
    }
   ],
   "source": [
    "ner = spacy.load(\"en_core_web_sm\")\n",
    "doc = \"Dr.Sumit is an adjunct faculty at UpGrad.\"\n",
    "processed_doc = ner(doc)\n",
    "for token in processed_doc:\n",
    "    print(token.text, token.pos_, token.ent_iob_, token.ent_type_)\n",
    "print(\"--------------------------\")\n",
    "for ents in processed_doc.ents:\n",
    "    print(ents.text, ents.start_char, ents.end_char, ents.label_)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anonymizing of the data using spacy\n",
    "\n",
    "email = ('Dear Family, Jose Luis and I have changed our dates, we are '\n",
    "         'going to come to Aspen on the 23rd of December and leave on the '\n",
    "         '30th of December. We would like to stay in the front bedroom of '\n",
    "         'the Aspen Cottage so that Mark, Natalie and Zachary can stay in '\n",
    "         'the guest cottage. Please let me know if there are any problems '\n",
    "         'with this. If I do not hear anything, I will assume this is all '\n",
    "         'o.k. with you.'\n",
    "         'Love, Liz')\n",
    "\n",
    "processed_email = ner(email) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Family, ********* and I have changed our dates, we are going to come to Aspen on ******************** and leave on ********************. We would like to stay in the front bedroom of the Aspen Cottage so that ****, ******* and ******* can stay in the guest cottage. Please let me know if there are any problems with this. If I do not hear anything, I will assume this is all o.k. with you.Love, ***'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anonymized_email = list(email)\n",
    "for ent in processed_email.ents:\n",
    "  if(ent.label_ == 'PERSON') or ent.label_ == 'DATE':\n",
    "    for char_pos in range(ent.start_char, ent.end_char):\n",
    "      anonymized_email[char_pos] = '*'\n",
    "\n",
    "\"\".join(anonymized_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Random Fields CRF\n",
    "label_test = open(\"F:\\programming\\python-development\\python-questions\\Database\\\\nlp_data\\label_test.txt\", 'rb').read().decode('utf-8').split(\"\\n\")\n",
    "label_train = open(\"F:\\programming\\python-development\\python-questions\\Database\\\\nlp_data\\label_train.txt\", 'rb').read().decode('utf-8').split(\"\\n\")\n",
    "sent_test = open(\"F:\\programming\\python-development\\python-questions\\Database\\\\nlp_data\\sent_test.txt\", 'rb').read().decode('utf-8').split(\"\\n\")\n",
    "sent_train = open(\"F:\\programming\\python-development\\python-questions\\Database\\\\nlp_data\\sent_train.txt\", 'rb').read().decode('utf-8').split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycrf\n",
    "# !pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f1 = input word is in lower case;\n",
    "- f2 = last 3 characters of word;\n",
    "- f3 = last 2 characers of word;\n",
    "- f4 = 1; if the word is in uppercase, 0 otherwise;\n",
    "- f5 = 1; if word is a number; otherwise, 0\n",
    "- f6= 1; if the word starts with a capital letter; otherwise, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_word(sentence, pos):\n",
    "    word = sentence[pos]\n",
    "    features = [\n",
    "    'word.lower = ' + word.lower(), # serves as word id\n",
    "    'word[-3:] = ' + word[-3:],     # last three characters\n",
    "    'word[-2:] = ' + word[-2:],     # last two characters\n",
    "    'word.isupper = %s' % word.isupper(),  # is the word in all uppercase\n",
    "    'word.isdigit = %s' % word.isdigit(),  # is the word a number\n",
    "    'words.startsWithCapital = %s' % word[0].isupper() # is the word starting with a capital letter\n",
    "    ] \n",
    "\n",
    "    if(pos > 0):\n",
    "        prev_word = sentence[pos-1]\n",
    "        features.extend([\n",
    "            'prev_word.lower = ' + prev_word.lower(),\n",
    "            'prev_word.isupper = %s' % prev_word.isupper(),\n",
    "            'prev_word.isdigit = %s' % prev_word.isdigit(),\n",
    "            'prev_word.startsWithCapital = %s' % prev_word[0].isupper()\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BEG') # feature to track begin of sentence \n",
    "\n",
    "    if(pos == len(sentence)-1):\n",
    "        features.append('END') # feature to track end of sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_features_for_sentence(sentence):\n",
    "    sentence_list = sentence.split()\n",
    "    return [get_features_for_word(sentence, i) for i in range(len(sentence_list))]\n",
    "\n",
    "def get_labels_for_sentence(labels):\n",
    "    return labels.split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
